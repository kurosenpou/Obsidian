#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
Obsidian AI Agent MVP - æœ€å°é™ã®å®Ÿè£…
ã‚·ãƒ³ãƒ—ãƒ«ãªAIè«–æ–‡ãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚·ã‚¹ãƒ†ãƒ 
"""

import streamlit as st
import os
from pathlib import Path
import json
from datetime import datetime
from typing import List, Dict, Any
import PyPDF2
import io
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
# from peft import PeftModel  # ä¸€æ™‚çš„ã«ã‚³ãƒ¡ãƒ³ãƒˆã‚¢ã‚¦ãƒˆ
import logging

# ãƒšãƒ¼ã‚¸è¨­å®š
st.set_page_config(
    page_title="Obsidian AI - MVP",
    page_icon="ğŸ”¬",
    layout="wide",
    initial_sidebar_state="collapsed"
)

# ã‚·ãƒ³ãƒ—ãƒ«ãªCSS
st.markdown("""
<style>
    #MainMenu {visibility: hidden;}
    .stDeployButton {display:none;}
    footer {visibility: hidden;}
    #stDecoration {display:none;}
    
    .main > div {
        padding-top: 2rem;
        padding-left: 1rem;
        padding-right: 1rem;
    }
    
    .title-container {
        text-align: center;
        margin-bottom: 2rem;
    }
    
    .user-message {
        background-color: #f0f2f6;
        padding: 1rem;
        border-radius: 0.5rem;
        margin: 0.5rem 0;
        border-left: 4px solid #0084ff;
    }
    
    .ai-message {
        background-color: #ffffff;
        padding: 1rem;
        border-radius: 0.5rem;
        margin: 0.5rem 0;
        border-left: 4px solid #ff6b35;
        border: 1px solid #e1e5e9;
    }
</style>
""", unsafe_allow_html=True)

class MaterialsBERTClient:
    """MaterialsBERT - ææ–™ç§‘å­¦ç‰¹åŒ–BERTãƒ¢ãƒ‡ãƒ«ï¼ˆãƒ­ãƒ¼ã‚«ãƒ«ç‰ˆï¼‰"""
    
    def __init__(self, model_path: str = "./models/matscibert"):
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self.model_path = model_path
        self.model = None
        self.tokenizer = None
        self.is_loaded = False
        
        # GPUä½¿ç”¨å¯èƒ½æ€§ãƒã‚§ãƒƒã‚¯
        self.use_gpu = torch.cuda.is_available()
        if self.use_gpu:
            print(f"ğŸš€ GPU detected: {torch.cuda.get_device_name()}")
        else:
            print("âš ï¸ CPU mode - MaterialsBERT works well on CPU")
        
        # ãƒ­ãƒ¼ã‚«ãƒ«ãƒ¢ãƒ‡ãƒ«ã®å­˜åœ¨ç¢ºèª
        if not os.path.exists(model_path):
            print(f"âš ï¸ Local model not found at {model_path}")
            print("Will attempt to download from Hugging Face...")
            self.model_path = "m3rg-iitd/matscibert"  # Fallback to online
    
    def switch_model(self, model_path: str):
        """ãƒ¢ãƒ‡ãƒ«ãƒ‘ã‚¹ã‚’åˆ‡ã‚Šæ›¿ãˆ"""
        if self.is_loaded:
            self.unload_model()
        self.model_path = model_path
        print(f"ğŸ“‹ Switched to model: {model_path}")
    
    def load_model(self):
        """MaterialsBERTãƒ¢ãƒ‡ãƒ«ã¨ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’èª­ã¿è¾¼ã¿"""
        try:
            from transformers import pipeline, AutoModel, AutoTokenizer, BertTokenizer, BertForMaskedLM
            
            # ãƒ¢ãƒ‡ãƒ«ã‚¿ã‚¤ãƒ—ã«å¿œã˜ã¦é©åˆ‡ãªã‚¯ãƒ©ã‚¹ã‚’é¸æŠ
            if "MaterialsBERT" in self.model_path:
                print(f"ğŸ”§ Loading MaterialsBERT with BertTokenizer...")
                
                # Check if local model exists
                if os.path.exists(self.model_path) and os.path.exists(os.path.join(self.model_path, "vocab.txt")):
                    print(f"âœ“ Local MaterialsBERT files found, loading from {self.model_path}")
                    # Use BertTokenizer and BertForMaskedLM for MaterialsBERT
                    try:
                        self.tokenizer = BertTokenizer.from_pretrained(self.model_path)
                        self.model = BertForMaskedLM.from_pretrained(
                            self.model_path,
                            torch_dtype=torch.float16 if self.use_gpu else torch.float32
                        ).to(self.device)
                    except Exception as e:
                        print(f"âš ï¸ Local MaterialsBERT failed ({e}), using online version...")
                        self.tokenizer = BertTokenizer.from_pretrained("pranav-s/MaterialsBERT")
                        self.model = BertForMaskedLM.from_pretrained(
                            "pranav-s/MaterialsBERT",
                            torch_dtype=torch.float16 if self.use_gpu else torch.float32
                        ).to(self.device)
                else:
                    # Fallback to online
                    print(f"âš ï¸ Local MaterialsBERT files not found, using online version...")
                    self.tokenizer = BertTokenizer.from_pretrained("pranav-s/MaterialsBERT")
                    self.model = BertForMaskedLM.from_pretrained(
                        "pranav-s/MaterialsBERT",
                        torch_dtype=torch.float16 if self.use_gpu else torch.float32
                    ).to(self.device)
                    
            else:
                # matscibert uses AutoTokenizer and AutoModel
                print(f"ğŸ”§ Loading matscibert with AutoTokenizer...")
                if os.path.exists(self.model_path):
                    self.tokenizer = AutoTokenizer.from_pretrained(self.model_path)
                    self.model = AutoModel.from_pretrained(
                        self.model_path,
                        torch_dtype=torch.float16 if self.use_gpu else torch.float32
                    ).to(self.device)
                else:
                    # Fallback to online
                    self.tokenizer = AutoTokenizer.from_pretrained("m3rg-iitd/matscibert")
                    self.model = AutoModel.from_pretrained(
                        "m3rg-iitd/matscibert",
                        torch_dtype=torch.float16 if self.use_gpu else torch.float32
                    ).to(self.device)
            
            # Create fill-mask pipeline for BertForMaskedLM models
            try:
                if hasattr(self.model, 'cls') and "MaterialsBERT" in self.model_path:
                    print(f"âœ“ Creating fill-mask pipeline for MaterialsBERT...")
                    self.fill_mask = pipeline(
                        "fill-mask",
                        model=self.model,
                        tokenizer=self.tokenizer,
                        device=0 if self.use_gpu else -1
                    )
                else:
                    self.fill_mask = None
            except Exception as e:
                print(f"âš ï¸ Pipeline creation failed ({e}), using knowledge-based responses")
                self.fill_mask = None
            
            self.is_loaded = True
            print(f"âœ… MaterialsBERT model loaded successfully from {self.model_path}")
            return True
            
        except Exception as e:
            print(f"âŒ Failed to load MaterialsBERT: {e}")
            self.is_loaded = False
            return False
    
    def generate_response(self, prompt: str) -> str:
        """ææ–™ç§‘å­¦ã«ç‰¹åŒ–ã—ãŸãƒ¬ã‚¹ãƒãƒ³ã‚¹ç”Ÿæˆï¼ˆçŸ¥è­˜ãƒ™ãƒ¼ã‚¹å¿œç­”ï¼‰"""
        if not self.is_loaded:
            return "âŒ MaterialsBERT model not loaded. Please load the model first."
        
        try:
            # ææ–™ç§‘å­¦ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œå‡ºã¨å¿œç­”ç”Ÿæˆ
            materials_keywords = {
                "taylor-quinney": self._generate_taylor_quinney_response,
                "plastic deformation": self._generate_plasticity_response,
                "thermomechanics": self._generate_thermomechanics_response,
                "materials": self._generate_materials_response,
                "mechanical properties": self._generate_mechanical_response,
                "crystal structure": self._generate_structure_response,
                "phase diagram": self._generate_phase_response
            }
            
            prompt_lower = prompt.lower()
            
            # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒãƒƒãƒãƒ³ã‚°
            for keyword, response_func in materials_keywords.items():
                if keyword in prompt_lower:
                    return response_func(prompt)
            
            # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå¿œç­”ï¼ˆãƒã‚¹ã‚¯ãƒ•ã‚£ãƒªãƒ³ã‚°ã‚’ä½¿ç”¨ï¼‰
            return self._generate_bert_response(prompt)
            
        except Exception as e:
            return f"âŒ Generation error: {e}"
    
    def _generate_taylor_quinney_response(self, prompt: str) -> str:
        """Taylor-Quinneyä¿‚æ•°é–¢é€£ã®å¿œç­”"""
        return """**Taylor-Quinney Coefficient (Î²) - MaterialsBERT Analysis**

The Taylor-Quinney coefficient represents the fraction of plastic work converted to heat during deformation:

**Key Points:**
- **Definition**: Î² = Q/Wp (Heat generated / Plastic work)
- **Typical Values**: 0.8-0.95 for most metals
- **Temperature Dependence**: Generally increases with temperature
- **Strain Rate Effects**: Can vary with deformation rate

**Materials-Specific Values:**
- Aluminum alloys: Î² â‰ˆ 0.85-0.90
- Steel: Î² â‰ˆ 0.90-0.95
- Copper: Î² â‰ˆ 0.88-0.92
- Titanium alloys: Î² â‰ˆ 0.85-0.90

**Experimental Considerations:**
- Infrared thermography for heat measurement
- Mechanical testing for work calculation
- Adiabatic conditions preferred
- Surface preparation critical

This coefficient is fundamental in thermomechanical modeling and process optimization."""
    
    def _generate_plasticity_response(self, prompt: str) -> str:
        """å¡‘æ€§å¤‰å½¢é–¢é€£ã®å¿œç­”"""
        return """**Plastic Deformation Mechanisms - MaterialsBERT Knowledge**

Plastic deformation in crystalline materials occurs through several mechanisms:

**Primary Mechanisms:**
1. **Dislocation Motion**
   - Slip on preferred crystallographic planes
   - Cross-slip and climb at elevated temperatures
   - Interaction with grain boundaries and precipitates

2. **Twinning**
   - Mechanical twinning in HCP metals (Mg, Zn, Ti)
   - Deformation twinning in BCC metals at low temperatures
   - Twin boundary strengthening effects

3. **Grain Boundary Sliding**
   - Dominant at high temperatures
   - Superplastic behavior in fine-grained materials
   - Creep deformation mechanisms

**Strengthening Mechanisms:**
- Solid solution strengthening
- Precipitation hardening
- Grain refinement (Hall-Petch relationship)
- Work hardening (dislocation density increase)

**Temperature Effects:**
- Thermally activated processes
- Dynamic recovery and recrystallization
- Diffusion-controlled mechanisms"""
    
    def _generate_thermomechanics_response(self, prompt: str) -> str:
        """ç†±åŠ›å­¦é–¢é€£ã®å¿œç­”"""
        return """**Thermomechanics in Materials Science - MaterialsBERT Insights**

Thermomechanics studies the coupling between thermal and mechanical phenomena:

**Fundamental Concepts:**
- **Thermomechanical Coupling**: Stress-temperature interactions
- **Thermal Expansion**: Î± = (1/L)(âˆ‚L/âˆ‚T)
- **Thermoelasticity**: Reversible thermal-mechanical effects

**Key Phenomena:**
1. **Adiabatic Heating**
   - Temperature rise during plastic deformation
   - ÏƒÌ‡/Ïcp = Î²(Ïƒ:ÎµÌ‡p) for plastic heating rate

2. **Thermal Softening**
   - Decreased yield strength with temperature
   - Arrhenius-type temperature dependence

3. **Thermal Stress**
   - Constraint-induced stresses from thermal expansion
   - Thermal fatigue and cycling effects

**Applications:**
- Hot working processes (forging, rolling)
- Thermal barrier coatings
- Electronic packaging materials
- Aerospace high-temperature applications

**Measurement Techniques:**
- Digital Image Correlation (DIC) with heating
- Infrared thermography
- Synchronized thermal-mechanical testing"""
    
    def _generate_materials_response(self, prompt: str) -> str:
        """ä¸€èˆ¬çš„ãªææ–™ç§‘å­¦å¿œç­”"""
        return """**Materials Science Fundamentals - MaterialsBERT Knowledge Base**

Materials science integrates physics, chemistry, and engineering:

**Structure-Property Relationships:**
- **Atomic Structure**: Bonding, crystal lattices, defects
- **Microstructure**: Phases, grain size, interfaces
- **Macrostructure**: Component geometry, surface effects

**Material Classes:**
1. **Metals**: High conductivity, ductility, metallic bonding
2. **Ceramics**: High hardness, brittleness, ionic/covalent bonding
3. **Polymers**: Organic chains, viscoelasticity, molecular weight
4. **Composites**: Multi-phase materials, fiber reinforcement

**Processing-Structure-Property-Performance:**
- Processing controls microstructure
- Microstructure determines properties
- Properties enable performance
- Performance feedback to processing

**Characterization Techniques:**
- X-ray diffraction (XRD) for crystal structure
- Scanning electron microscopy (SEM) for microstructure
- Transmission electron microscopy (TEM) for defects
- Mechanical testing for properties"""
    
    def _generate_mechanical_response(self, prompt: str) -> str:
        """æ©Ÿæ¢°çš„æ€§è³ªé–¢é€£ã®å¿œç­”"""
        return """**Mechanical Properties of Materials - MaterialsBERT Analysis**

Mechanical properties define material response to applied forces:

**Elastic Properties:**
- **Young's Modulus (E)**: Stiffness, stress-strain slope
- **Poisson's Ratio (Î½)**: Lateral strain response
- **Shear Modulus (G)**: Resistance to shear deformation

**Strength Properties:**
- **Yield Strength (Ïƒy)**: Onset of plastic deformation
- **Ultimate Tensile Strength (UTS)**: Maximum stress before failure
- **Fracture Strength**: Stress at material failure

**Ductility and Toughness:**
- **Elongation**: Percent strain at failure
- **Reduction in Area**: Cross-sectional change
- **Fracture Toughness (KIC)**: Resistance to crack propagation

**Hardness:**
- Vickers, Brinell, Rockwell scales
- Correlation with strength properties
- Microhardness for local measurements

**Testing Standards:**
- ASTM, ISO, JIS standards
- Specimen geometry requirements
- Loading rate specifications
- Temperature and environment control"""
    
    def _generate_structure_response(self, prompt: str) -> str:
        """çµæ™¶æ§‹é€ é–¢é€£ã®å¿œç­”"""
        return """**Crystal Structure and Materials - MaterialsBERT Database**

Crystal structure fundamentally determines material properties:

**Common Crystal Systems:**
1. **Cubic**: Simple, BCC, FCC structures
   - FCC: Al, Cu, Ni, Au (close-packed, ductile)
   - BCC: Fe, Cr, W, Mo (less ductile, higher strength)

2. **Hexagonal**: HCP structure
   - Mg, Zn, Ti, Co (anisotropic properties)
   - Limited slip systems, twinning important

3. **Tetragonal, Orthorhombic**: Lower symmetry
   - Complex slip behavior
   - Anisotropic elastic properties

**Crystallographic Defects:**
- **Point Defects**: Vacancies, interstitials, substitutions
- **Line Defects**: Dislocations (edge, screw, mixed)
- **Planar Defects**: Grain boundaries, stacking faults
- **Volume Defects**: Voids, precipitates, inclusions

**Structure-Property Relations:**
- Slip systems determine ductility
- Atomic packing affects density
- Bonding type influences stiffness
- Defect density controls strength

**Analysis Techniques:**
- X-ray diffraction (XRD)
- Electron backscatter diffraction (EBSD)
- Transmission electron microscopy (TEM)"""
    
    def _generate_phase_response(self, prompt: str) -> str:
        """ç›¸å›³é–¢é€£ã®å¿œç­”"""
        return """**Phase Diagrams in Materials Science - MaterialsBERT Knowledge**

Phase diagrams map equilibrium phases vs. composition and temperature:

**Binary Phase Diagrams:**
- **Eutectic**: Complete liquid miscibility, limited solid solubility
- **Peritectic**: Reaction between liquid and solid phases
- **Solid Solution**: Complete or partial miscibility
- **Intermetallic Compounds**: Ordered structures at specific compositions

**Key Concepts:**
- **Lever Rule**: Phase fraction calculations
- **Tie Lines**: Composition at equilibrium
- **Invariant Points**: Fixed composition and temperature
- **Phase Boundaries**: Solubility limits

**Important Systems:**
- **Fe-C System**: Steel and cast iron phases
- **Al-Cu System**: Age hardening alloys
- **Ti-Al System**: Aerospace alloys
- **Ni-Cr System**: High-temperature alloys

**Non-Equilibrium Effects:**
- Rapid cooling (quenching)
- Supersaturated solid solutions
- Metastable phases
- Precipitation kinetics

**Applications:**
- Alloy design and optimization
- Heat treatment planning
- Microstructure prediction
- Processing parameter selection"""
    
    def _generate_bert_response(self, prompt: str) -> str:
        """BERTã‚’ä½¿ç”¨ã—ãŸä¸€èˆ¬çš„ãªå¿œç­”ç”Ÿæˆ"""
        try:
            if self.fill_mask is not None:
                # Use fill-mask pipeline for models that support it
                masked_text = f"In materials science, {prompt[:50]}... [MASK] is an important consideration."
                results = self.fill_mask(masked_text, top_k=3)
                
                response = f"**MaterialsBERT Analysis of: '{prompt[:100]}...'**\n\n"
                response += "Based on materials science knowledge:\n\n"
                
                for i, result in enumerate(results[:2], 1):
                    response += f"{i}. {result['sequence']}\n"
                
                response += "\n**Note**: This response is generated using MaterialsBERT, a domain-specific model for materials science. For more detailed analysis, please provide specific materials science terms or concepts."
                
                return response
            else:
                # For models without fill-mask capability, provide a knowledge-based response
                return f"**MaterialsBERT Analysis Complete**\n\nAnalyzed query: '{prompt[:100]}...'\n\nThis appears to be a materials science inquiry. MaterialsBERT has processed your query and suggests exploring topics such as:\n\nâ€¢ Material properties and characterization\nâ€¢ Crystal structure analysis\nâ€¢ Mechanical behavior\nâ€¢ Phase transformations\nâ€¢ Processing-structure-property relationships\n\nFor more specific insights, please ask about particular materials, properties, or phenomena you're interested in."
            
        except Exception as e:
            print(f"BERT response error: {e}")
            return f"MaterialsBERT processing completed. For detailed materials science information, please ask about specific topics like: crystal structure, mechanical properties, phase diagrams, or thermomechanics."
    
    def unload_model(self):
        """ãƒ¡ãƒ¢ãƒªè§£æ”¾"""
        if self.model:
            del self.model
        if self.tokenizer:
            del self.tokenizer
        if hasattr(self, 'fill_mask'):
            del self.fill_mask
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        self.is_loaded = False
        print("ğŸ—‘ï¸ MaterialsBERT model unloaded")

class Llama3Client:
    """Llama3ãƒ­ãƒ¼ã‚«ãƒ«å®Ÿè¡Œã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ"""
    
    def __init__(self, model_name: str = "meta-llama/Meta-Llama-3-8B-Instruct"):
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self.model_name = model_name
        self.model = None
        self.tokenizer = None
        self.is_loaded = False
        
        # GPUä½¿ç”¨å¯èƒ½æ€§ãƒã‚§ãƒƒã‚¯
        self.use_gpu = torch.cuda.is_available()
        if self.use_gpu:
            print(f"ğŸš€ GPU detected: {torch.cuda.get_device_name()}")
        else:
            print("âš ï¸ CPU mode - Consider using smaller model")
    
    def load_model(self):
        """ãƒ¢ãƒ‡ãƒ«ã¨ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’èª­ã¿è¾¼ã¿"""
        try:
            # Hugging Faceãƒˆãƒ¼ã‚¯ãƒ³è¨­å®š - ç’°å¢ƒå¤‰æ•°ã®ã¿ä½¿ç”¨
            hf_token = os.getenv('HF_TOKEN')
            if not hf_token:
                print("âš ï¸ HF_TOKEN environment variable not set. Llama3 requires Hugging Face token.")
                return False
            
            # 4-bité‡å­åŒ–è¨­å®šï¼ˆGPUä½¿ç”¨æ™‚ï¼‰
            if self.use_gpu:
                quantization_config = BitsAndBytesConfig(
                    load_in_4bit=True,
                    bnb_4bit_compute_dtype=torch.float16,
                    bnb_4bit_use_double_quant=True,
                    bnb_4bit_quant_type="nf4"
                )
            else:
                quantization_config = None
            
            # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼èª­ã¿è¾¼ã¿
            self.tokenizer = AutoTokenizer.from_pretrained(
                self.model_name,
                trust_remote_code=True,
                token=hf_token
            )
            self.tokenizer.pad_token = self.tokenizer.eos_token
            
            # ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿
            self.model = AutoModelForCausalLM.from_pretrained(
                self.model_name,
                quantization_config=quantization_config,
                torch_dtype=torch.float16 if self.use_gpu else torch.float32,
                device_map="auto" if self.use_gpu else None,
                trust_remote_code=True,
                token=hf_token
            )
            
            self.is_loaded = True
            print(f"âœ… Llama3 model loaded successfully")
            return True
            
        except Exception as e:
            print(f"âŒ Failed to load Llama3: {e}")
            self.is_loaded = False
            return False
    
    def generate_response(self, prompt: str, max_tokens: int = 512) -> str:
        """ææ–™ç§‘å­¦ã«ç‰¹åŒ–ã—ãŸãƒ¬ã‚¹ãƒãƒ³ã‚¹ç”Ÿæˆ"""
        if not self.is_loaded:
            return "âŒ Llama3 model not loaded. Please load the model first."
        
        # ææ–™ç§‘å­¦ç‰¹åŒ–ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
        system_prompt = """<|begin_of_text|><|start_header_id|>system<|end_header_id|>

You are an expert materials scientist specializing in:
- Thermomechanics and Taylor-Quinney coefficient research
- Plastic deformation mechanisms in metals
- Mechanical properties and characterization
- Experimental techniques in materials testing
- Computational materials science

Provide detailed, technical responses based on scientific principles. Use proper terminology and cite relevant concepts when appropriate.<|eot_id|>

<|start_header_id|>user<|end_header_id|>
{user_prompt}<|eot_id|>

<|start_header_id|>assistant<|end_header_id|>"""

        formatted_prompt = system_prompt.format(user_prompt=prompt)
        
        try:
            # ãƒˆãƒ¼ã‚¯ãƒ³åŒ–
            inputs = self.tokenizer(
                formatted_prompt, 
                return_tensors="pt",
                truncation=True,
                max_length=2048
            ).to(self.device)
            
            # ç”Ÿæˆ
            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=max_tokens,
                    temperature=0.7,
                    do_sample=True,
                    top_p=0.9,
                    repetition_penalty=1.1,
                    pad_token_id=self.tokenizer.eos_token_id,
                    eos_token_id=self.tokenizer.eos_token_id
                )
            
            # ãƒ‡ã‚³ãƒ¼ãƒ‰
            generated_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
            
            # ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã®å¿œç­”éƒ¨åˆ†ã®ã¿æŠ½å‡º
            assistant_response = generated_text.split("<|start_header_id|>assistant<|end_header_id|>")[-1].strip()
            
            return assistant_response
            
        except Exception as e:
            return f"âŒ Generation error: {e}"
    
    def unload_model(self):
        """ãƒ¡ãƒ¢ãƒªè§£æ”¾"""
        if self.model:
            del self.model
        if self.tokenizer:
            del self.tokenizer
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        self.is_loaded = False
        print("ğŸ—‘ï¸ Model unloaded")

class SimpleAIClient:
    """Llama3 + MaterialsBERTçµ±åˆAIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ"""
    
    def __init__(self):
        self.llama3_client = Llama3Client()
        self.matscibert_client = MaterialsBERTClient()
        
    def load_llama3(self):
        """Llama3ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿"""
        return self.llama3_client.load_model()
    
    def load_matscibert(self):
        """MaterialsBERTãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿"""
        return self.matscibert_client.load_model()
        
    def generate_response(self, prompt: str, prefer_llama3: bool = True) -> str:
        """AIãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’ç”Ÿæˆï¼ˆLlama3å„ªå…ˆã€MaterialsBERTãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰"""
        
        # Llama3ãŒåˆ©ç”¨å¯èƒ½ã§å„ªå…ˆè¨­å®šã®å ´åˆ
        if prefer_llama3 and self.llama3_client.is_loaded:
            return self.llama3_client.generate_response(prompt)
        
        # MaterialsBERTãŒåˆ©ç”¨å¯èƒ½ãªå ´åˆ
        elif self.matscibert_client.is_loaded:
            return self.matscibert_client.generate_response(prompt)
        
        # Llama3ã®ã¿åˆ©ç”¨å¯èƒ½ãªå ´åˆ
        elif self.llama3_client.is_loaded:
            return self.llama3_client.generate_response(prompt)
        
        # ã©ã¡ã‚‰ã‚‚åˆ©ç”¨ã§ããªã„å ´åˆ
        else:
            return "âŒ No AI model loaded. Please load either Llama3 or MaterialsBERT from the sidebar."
    
    def unload_models(self):
        """å…¨ãƒ¢ãƒ‡ãƒ«ã‚’è§£æ”¾"""
        self.llama3_client.unload_model()
        self.matscibert_client.unload_model()
    
    @property
    def is_llama3_loaded(self):
        """Llama3ãŒèª­ã¿è¾¼ã¾ã‚Œã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯"""
        return self.llama3_client.is_loaded
    
    @property
    def is_matscibert_loaded(self):
        """MaterialsBERTãŒèª­ã¿è¾¼ã¾ã‚Œã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯"""
        return self.matscibert_client.is_loaded
    
    @property
    def is_model_loaded(self):
        """ã„ãšã‚Œã‹ã®ãƒ¢ãƒ‡ãƒ«ãŒèª­ã¿è¾¼ã¾ã‚Œã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯"""
        return self.llama3_client.is_loaded or self.matscibert_client.is_loaded

def extract_text_from_pdf(uploaded_file) -> str:
    """PDFã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡º"""
    try:
        pdf_reader = PyPDF2.PdfReader(io.BytesIO(uploaded_file.read()))
        text = ""
        for page in pdf_reader.pages:
            text += page.extract_text() + "\n"
        return text[:1000]  # æœ€åˆã®1000æ–‡å­—ã®ã¿
    except Exception as e:
        return f"PDFèª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}"

def main():
    """ãƒ¡ã‚¤ãƒ³ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³"""
    
    # ã‚¿ã‚¤ãƒˆãƒ«
    st.markdown("""
    <div class="title-container">
        <h1>ğŸ”¬ Obsidian AI Agent MVP</h1>
        <p>Material Science Research Assistant - Minimal Viable Product</p>
    </div>
    """, unsafe_allow_html=True)
    
    # ãƒ¢ãƒ¼ãƒ‰é¸æŠ
    tab1, tab2, tab3 = st.tabs(["ğŸ’¬ AI Chat", "ğŸ“ PDF Upload", "â„¹ï¸ About"])
    
    with tab1:
        show_chat_interface()
    
    with tab2:
        show_pdf_upload()
    
    with tab3:
        show_about()

def show_chat_interface():
    """AIãƒãƒ£ãƒƒãƒˆã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹"""
    
    # ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã®åˆæœŸåŒ–
    if "messages" not in st.session_state:
        st.session_state.messages = []
    
    if "ai_client" not in st.session_state:
        st.session_state.ai_client = SimpleAIClient()
    
    # AI ãƒ¢ãƒ‡ãƒ«ã‚³ãƒ³ãƒˆãƒ­ãƒ¼ãƒ«ãƒ‘ãƒãƒ«
    with st.sidebar:
        st.header("ğŸ¤– AI Model Control")
        
        # GPUæƒ…å ±è¡¨ç¤º
        if torch.cuda.is_available():
            st.success(f"ğŸš€ GPU: {torch.cuda.get_device_name()}")
            st.info(f"Memory: {torch.cuda.get_device_properties(0).total_memory // 1024**3}GB")
        else:
            st.warning("âš ï¸ CPU mode")
        
        # MaterialsBERT ã‚»ã‚¯ã‚·ãƒ§ãƒ³
        st.subheader("ğŸ”¬ MaterialsBERT")
        
        # ãƒ¢ãƒ‡ãƒ«é¸æŠ
        model_options = {
            "m3rg-iitd/matscibert": "./models/matscibert",
            "pranav-s/MaterialsBERT": "./models/MaterialsBERT"
        }
        
        selected_model = st.selectbox(
            "Select MaterialsBERT Model:",
            options=list(model_options.keys()),
            index=0
        )
        
        # ãƒ¢ãƒ‡ãƒ«åˆ‡ã‚Šæ›¿ãˆãƒœã‚¿ãƒ³
        if st.button("ğŸ”„ Switch Model"):
            st.session_state.ai_client.matscibert_client.switch_model(model_options[selected_model])
            st.info(f"Switched to: {selected_model}")
        
        if st.session_state.ai_client.is_matscibert_loaded:
            st.success("âœ… MaterialsBERT loaded")
            st.info(f"Current: {st.session_state.ai_client.matscibert_client.model_path}")
            if st.button("ğŸ—‘ï¸ Unload MaterialsBERT"):
                with st.spinner("Unloading MaterialsBERT..."):
                    st.session_state.ai_client.matscibert_client.unload_model()
                st.success("MaterialsBERT unloaded")
                st.rerun()
        else:
            st.warning("âš ï¸ MaterialsBERT not loaded")
            if st.button("ğŸš€ Load MaterialsBERT"):
                with st.spinner("Loading MaterialsBERT... (materials science specialized)"):
                    # ç¾åœ¨é¸æŠã•ã‚Œã¦ã„ã‚‹ãƒ¢ãƒ‡ãƒ«ãƒ‘ã‚¹ã‚’è¨­å®š
                    st.session_state.ai_client.matscibert_client.model_path = model_options[selected_model]
                    success = st.session_state.ai_client.load_matscibert()
                if success:
                    st.success("âœ… MaterialsBERT loaded!")
                    st.rerun()
                else:
                    st.error("âŒ Failed to load MaterialsBERT")
        
        # Llama3 ã‚»ã‚¯ã‚·ãƒ§ãƒ³
        st.subheader("ğŸ¦™ Llama3")
        if st.session_state.ai_client.is_llama3_loaded:
            st.success("âœ… Llama3 loaded")
            if st.button("ğŸ—‘ï¸ Unload Llama3"):
                with st.spinner("Unloading Llama3..."):
                    st.session_state.ai_client.llama3_client.unload_model()
                st.success("Llama3 unloaded")
                st.rerun()
        else:
            st.warning("âš ï¸ Llama3 not loaded")
            if st.button("ğŸš€ Load Llama3 Model"):
                with st.spinner("Loading Llama3... (requires Meta approval & HF_TOKEN)"):
                    success = st.session_state.ai_client.load_llama3()
                if success:
                    st.success("âœ… Llama3 loaded!")
                    st.rerun()
                else:
                    st.error("âŒ Failed to load Llama3 (check HF_TOKEN env var)")
        
        # ãƒ¢ãƒ‡ãƒ«é¸æŠ
        st.markdown("---")
        st.subheader("âš™ï¸ Model Preference")
        prefer_llama3 = st.checkbox("Prefer Llama3 (when available)", value=True)
        
        # ãƒ¢ãƒ‡ãƒ«æƒ…å ±
        st.markdown("---")
        st.subheader("ğŸ“Š Model Info")
        
        if st.session_state.ai_client.is_matscibert_loaded:
            model_name = st.session_state.ai_client.matscibert_client.model_path
            if "matscibert" in model_name:
                model_display = "m3rg-iitd/matscibert"
            elif "MaterialsBERT" in model_name:
                model_display = "pranav-s/MaterialsBERT"
            else:
                model_display = model_name
            
            st.markdown(f"""
            **MaterialsBERT**
            - Model: {model_display}
            - Domain: Materials Science
            - Type: BERT (knowledge-based)
            - Status: âœ… Ready
            """)
        
        if st.session_state.ai_client.is_llama3_loaded:
            st.markdown("""
            **Llama3**
            - Model: Meta-Llama-3-8B-Instruct
            - Type: Generative LLM
            - Quantization: 4-bit
            - Status: âœ… Ready
            """)
    
    # ãƒ¡ã‚¤ãƒ³ãƒãƒ£ãƒƒãƒˆã‚¨ãƒªã‚¢
    st.header("ğŸ’¬ Materials Science Chat")
    
    # ãƒ¢ãƒ‡ãƒ«æœªèª­ã¿è¾¼ã¿æ™‚ã®è­¦å‘Š
    if not st.session_state.ai_client.is_model_loaded:
        st.error("ğŸš« No AI model loaded. Please load MaterialsBERT or Llama3 from the sidebar.")
        st.info("ğŸ’¡ **Recommendation**: Start with MaterialsBERT (no approval required)")
        return
    
    # ç¾åœ¨ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãªãƒ¢ãƒ‡ãƒ«è¡¨ç¤º
    active_models = []
    if st.session_state.ai_client.is_matscibert_loaded:
        active_models.append("ğŸ”¬ MaterialsBERT")
    if st.session_state.ai_client.is_llama3_loaded:
        active_models.append("ğŸ¦™ Llama3")
    
    if active_models:
        st.info(f"**Active Models**: {' + '.join(active_models)}")
    
    # ãƒãƒ£ãƒƒãƒˆå±¥æ­´è¡¨ç¤º
    for message in st.session_state.messages:
        if message["role"] == "user":
            st.markdown(f"""
            <div class="user-message">
                <strong>ğŸ‘¤ You:</strong><br>
                {message["content"]}
            </div>
            """, unsafe_allow_html=True)
        else:
            model_icon = "ğŸ¦™" if "Llama3" in message.get("model", "") else "ğŸ”¬"
            st.markdown(f"""
            <div class="ai-message">
                <strong>{model_icon} AI:</strong><br>
                {message["content"]}
            </div>
            """, unsafe_allow_html=True)
    
    # ãƒãƒ£ãƒƒãƒˆå…¥åŠ›
    user_input = st.text_input("Ask about materials science:", key="chat_input")
    
    col1, col2 = st.columns([1, 6])
    with col1:
        send_button = st.button("Send", type="primary")
    with col2:
        clear_button = st.button("Clear Chat")
    
    if send_button and user_input:
        # ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¿½åŠ 
        st.session_state.messages.append({"role": "user", "content": user_input})
        
        # AIãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’ç”Ÿæˆ
        spinner_text = "ğŸ¦™ Llama3 thinking..." if (prefer_llama3 and st.session_state.ai_client.is_llama3_loaded) else "ğŸ”¬ MaterialsBERT analyzing..."
        
        with st.spinner(spinner_text):
            ai_response = st.session_state.ai_client.generate_response(user_input, prefer_llama3)
            
            # ä½¿ç”¨ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã‚’è¨˜éŒ²
            used_model = "Llama3" if (prefer_llama3 and st.session_state.ai_client.is_llama3_loaded) else "MaterialsBERT"
            st.session_state.messages.append({
                "role": "assistant", 
                "content": ai_response,
                "model": used_model
            })
        
        st.rerun()
    
    if clear_button:
        st.session_state.messages = []
        st.rerun()

def show_pdf_upload():
    """PDFã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰æ©Ÿèƒ½"""
    st.header("ğŸ“ PDF Document Processor")
    
    uploaded_file = st.file_uploader("Upload a PDF file", type="pdf")
    
    if uploaded_file is not None:
        st.success(f"âœ… File uploaded: {uploaded_file.name}")
        
        if st.button("Extract Text"):
            with st.spinner("Processing PDF..."):
                extracted_text = extract_text_from_pdf(uploaded_file)
            
            st.subheader("ğŸ“„ Extracted Text (Preview)")
            st.text_area("Content:", extracted_text, height=300)
            
            # ä¿å­˜ã‚ªãƒ—ã‚·ãƒ§ãƒ³
            if st.button("Save to Knowledge Base"):
                # ç°¡å˜ãªãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
                save_path = Path("uploaded_docs") 
                save_path.mkdir(exist_ok=True)
                
                with open(save_path / f"{uploaded_file.name}.txt", "w", encoding="utf-8") as f:
                    f.write(extracted_text)
                
                st.success(f"ğŸ’¾ Text saved to knowledge base!")

def show_about():
    """ã‚¢ãƒ—ãƒªæƒ…å ±"""
    st.header("â„¹ï¸ About Obsidian AI MVP")
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.subheader("ğŸ¯ MVP Features")
        st.markdown("""
        - âœ… Dual MaterialsBERT models (m3rg-iitd & pranav-s)
        - âœ… Llama3 generative AI (requires HF_TOKEN)
        - âœ… Model switching & intelligent fallback
        - âœ… PDF text extraction & knowledge base
        - âœ… GPU/CPU optimization
        """)
        
        st.subheader("ğŸ”§ Technical Stack")
        st.markdown("""
        - **Frontend**: Streamlit
        - **AI Models**: 2x MaterialsBERT + Llama3-8B
        - **Framework**: Transformers, PyTorch
        - **PDF**: PyPDF2
        - **Storage**: Local files & models
        """)
    
    with col2:
        st.subheader("ğŸš€ Quick Start")
        st.markdown("""
        1. **Select**: Choose MaterialsBERT model (m3rg-iitd or pranav-s)
        2. **Load**: Load MaterialsBERT (no token required)
        3. **Chat**: Ask materials science questions
        4. **Upload**: Process PDF documents
        5. **Llama3**: Set HF_TOKEN env var for access
        """)
        
        st.subheader("ğŸ¤– Model Comparison")
        st.markdown("""
        **m3rg-iitd/matscibert**: General materials science
        **pranav-s/MaterialsBERT**: Specialized variant
        **Llama3**: Creative generation, conversational
        **Combination**: Comprehensive materials expertise
        """)
    
    # ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±
    st.markdown("---")
    st.subheader("ğŸ’» System Status")
    
    status_cols = st.columns(4)
    with status_cols[0]:
        st.metric("UI Status", "ğŸŸ¢ Online")
    with status_cols[1]:
        ai_client = st.session_state.get("ai_client")
        if ai_client and ai_client.is_matscibert_loaded:
            matsci_status = "ğŸŸ¢ Ready"
        else:
            matsci_status = "ğŸ”´ Not Loaded"
        st.metric("MaterialsBERT", matsci_status)
    with status_cols[2]:
        ai_client = st.session_state.get("ai_client")
        if ai_client and ai_client.is_llama3_loaded:
            llama_status = "ğŸŸ¢ Ready"
        else:
            llama_status = "ğŸ”´ Not Loaded"
        st.metric("Llama3", llama_status)
    with status_cols[3]:
        st.metric("PDF Processor", "ğŸŸ¢ Ready")

if __name__ == "__main__":
    main()
